
* Introduction


the whole purpose of this system is to have distributed video and audio encoding, decoding and filtering.

The system operates by having master node and shared storage for data and other worker nodes to do processing with master node.
  
* Use cases

the usage of the system would be as follows:

#+begin_src bash
  disttranscode --input location --filter f1 param1 param2 --filter f2 param1 param2 output-location
#+end_src

and available filters which can be applied would be:

+ id width height - copy video
+ grayscale width height - convert video to grayscale
+ resize width height newwidth newheight - resize video
+ watermark width height x y text fontsize color - put watermark on video
+ image width height x y image-location - put image on whole video

 
* System overview 

** Overview

architecture pattern would be pipeline

** Components

 | Subsystem        | External dependencies | Consumes subsystem(s)                          | Provides service to subsystem(s)               | Access through   |
 |------------------+-----------------------+------------------------------------------------+------------------------------------------------+------------------|
 | argument parsing | argh                  |                                                | DataInputManager, FilterManager, OutputManager | observer pattern |
 | NodeManager      | OpenMPI               | FilterManager, DataInputManager, OutputManager | Filter plugin                                  | observer pattern |
 | other nodes      |                       |                                                |                                                | network          |
 | FilterManager    | dlopen                | argument parsing                               | NodeManager                                    | observer pattern |
 | DataInputManager | libav and ffmpeg      | argument parsing                               | NodeManager                                    | observer pattern |
 | OutputManager    | libav and ffmpeg      | argument parsing                               | NodeManager                                    | observer pattern |
 | Logging          | glog                  |                                                | all                                            | function call    |


+ NodeManager
  + keeps track of all nodes connected to master
  + when Stream is loaded, divides it into individual nodes for further processing
+ all components which access each other through observer pattern means that subsystem which PROVIDES service to subsystem X is actually providing Listener interface to X
+ for every filter managed by filter manager, individual plugins are loaded dynamically
+ DataInputManager and OutputManager have plugins for loading individual formats and outputing individual formats
  + internal representation is through Stream object



* Main concepts 

** Algorithms used 

in order to properly do division among nodes, we will need seek operation to be fast, this can be helpful

http://www.hackerfactor.com/blog/index.php?/archives/307-Picture-Go-Back.html

this can be used for splitting into segments

#+begin_src bash
  ffmpeg -i INPUT.mp4 -acodec copy -f segment -vcodec copy -reset_timestamps 1 -map 0 OUTPUT%d.mp4


#+end_src


*** Consensus

when writing output, all nodes keep track of last segment and they write their segments when it is their turn

since segments come in order, they can calculate when to write



TODO:

+ add broadcast to context as =operator<<= to send message to everyone
+ add possibility to receive message from anyone =context() >> message=
+ add =context.broadcast(rank) >> data= ability to broadcast message from process =rank= and to write it to data on all machines
+ Implement =FFMpegVideoDecoder= which inherits =VideoWriter= and has method =getSegment() -> Segment=
+ define =Consensus= interface and implement =UniformFloodingConsensus=
+ define =Distribution= interface =distribute: Stream -> IO= and =getRank: Int as index -> int as rank= and =next(): -> Int= 
+ implement =StrideDistribution(Context)=
+ Implement =Synchronizer= which has:
  + distribution implementation
  + consensus implementation
  + writer implementation
+ Synchronizer basically does:
  + takes stream
  + distributes it
  + repeatedly calls consensus
    + calls propose, in order to do that, it has to decode and transform and encode back frames and then proposes index of that segment
    + if we already have index of some segment which is computed already, we decode, transform and encode other segments and send this current index immediately
    + to get next segment to work on, call =distribution.next(context)= to get next index for given rank
    + retrieves next segment index by calling =getDecision=, if it is rank=0 then do the following:
      + from segment it calculates rank of process
      + gets segment from given process
      + writes it into file
    + if it is rank=0 or any other rank
      + if decided index is yours, send segment to 0
      + else do nothing


consensus


#+begin_example

  constructor(context):
	  round = 1
	  N = context.size()
	  decision = null
	  receivedfrom is array of integer ranks of size N
	  proposals is array of empty sets of size N
	  receivedfrom[0] = all processes ranks

  propose(value):
	  proposals[1].add(v)
	  context << "PROPOSAL" << 1 << proposals[1]

  getDecision():
      while decision == null:
	  for i = 0 to context.size():
	      int r, set ps
	      string msg
	      context(i) >> msg
	      if (msg == "PROPOSAL"):
		 context(i) >> r >> ps
		 receivedFrom[r].add(i)
		 proposals[r].union(ps)
		 if receivedFrom[round].size() == context.size() and decision == null:
		    if receivedFrom[round] == receivedFrom[round-1]:
		       decision = min(proposals[round])
		       context << "DECIDED" << decision
		       return decision
		    else:
		       round++
		       context << "PROPOSAL" << round << proposals[round-1]

	      else if msg == "DECIDED" and decision == null:
		   context(i) >> decision
		   context << decision
		   return decision
#+end_example

** Data structures used 

The stream will be represented as a vector of segments where every segment is actually list of frames



* Quality attriubutes 

** Adaptability 

+ new filters and input and output reading/writing can be added through plugin system
+ nodes can see other nodes in the network because of OpenMPI
   
** Configuration

+ nodes are configured the same way as any OpenMPI application
+ ini files for plugin configuration can be found on filesystem by specified rules
   
** Logging and debug system

+ all logging is done on console
+ debug flags are available through verbosity option with different debug levels (=--debug LEVEL=) where level can be any number from 0 to 10
   
** Demonstrators 

+ on Borat movie - creating grayscale Borat
+ Borat movie, inserting watermark
+ Comparing this system with single core ffmpeg implementation for turning video into grayscale and resizing
   
** Deployment 

+ release is deployed on GitHub with tag and releases page
   
** Durability and warranty 

+ none, software is MIT licensed
   
** Functional safety 

+ not applicable
   
** Security 

+ not applicable
   
** Scalability 

+ yes through OpenMPI, other aspects will be only tested
   
** Standards and certification compliance 

+ all audio and video standards will be covered through ffmpeg


* Data design 

** File formats 

| Data type            | Format             |
|----------------------+--------------------|
| Video                | avi, mkv, mp4      |
| Filter               | shared object file |
| data input reader    | shared object file |
| data output writer   | shared object file |
| plugin configuration | ini file           |


* Communication to external systems 

| External system | data shared                       | protocol |
|-----------------+-----------------------------------+----------|
| OpenMPI network | Segment objects and metadata info | MPI      |


* Logging and error processing 

** Log subsystem

+ Implemented using glog
   
** Error reporting , handling and recovery

+ fail fast and fail early mechanism
+ does not try to recover at all
+ all fatal errors will also bring other nodes to finalize as quickly as possible

* Operating system and hardware compatibility 

+ compatible with Linux on high variety of hardware
  
* Plugin system 

** Interface to core system 

+ filters are implemented as plugins where every plugin contains:
  + class inheriting Filter base class
  + class inheriting FilterFactory which will create given object
  + =create_filter_factory_instance= function which returns filter factory implemented

** Plugin installation and management 

+ just putting so file in particular directory
   
* Build system 

+ CMake will be used
  

* Profiling 

+ vargrind will be used for measurements
  
* Testing frameworks 

** Unit testing 

+ google test
   
** Integration testing 

+ creating specialized programs to test components
   
** System testing 

+ creating swarm of containers

* Code coverage tools 

* Static code analysis 

* Documentation handling 

* Existing technologies dependencies 

+ https://github.com/adishavit/argh
+ glog
+ google test
+ docker for system testing
+ libav
+ ffmpeg
+ OpenMPI
  
* References
